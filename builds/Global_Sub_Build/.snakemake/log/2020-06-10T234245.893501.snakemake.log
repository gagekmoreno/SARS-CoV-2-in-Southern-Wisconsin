Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	adjust_metadata
	1	aggregate_alignments
	1	all
	1	ancestral
	1	clades
	1	colors
	1	export
	1	filter
	1	fix_colorings
	1	incorporate_travel_history
	1	mask
	1	partition_sequences
	1	recency
	1	refine
	1	subsample
	1	tip_frequencies
	1	traits
	1	translate
	1	tree
	19

[Wed Jun 10 23:42:56 2020]
Job 19: 
        Filtering to
          - excluding strains in config/exclude.txt
          - minimum genome length of 25000
        


        augur filter             --sequences data/sequences.fasta             --metadata data/metadata.tsv             --include config/include.txt             --exclude config/exclude.txt             --exclude-where date='2020' date='2020-01-XX' date='2020-02-XX' date='2020-03-XX' date='2020-04-XX' date='2020-01' date='2020-02' date='2020-03' date='2020-04'            --min-length 25000             --group-by division year month             --sequences-per-group 4             --output results/filtered.fasta
        
[Wed Jun 10 23:43:16 2020]
Error in rule filter:
    jobid: 19
    output: results/filtered.fasta
    shell:
        
        augur filter             --sequences data/sequences.fasta             --metadata data/metadata.tsv             --include config/include.txt             --exclude config/exclude.txt             --exclude-where date='2020' date='2020-01-XX' date='2020-02-XX' date='2020-03-XX' date='2020-04-XX' date='2020-01' date='2020-02' date='2020-03' date='2020-04'            --min-length 25000             --group-by division year month             --sequences-per-group 4             --output results/filtered.fasta
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Terminating processes on user request, this might take some time.
Complete log: /nextstrain/build/.snakemake/log/2020-06-10T234245.893501.snakemake.log
